## モンテカルロ法による価値推定

モンテカルロ法とは、数値計算やシミュレーションなどにおいて、ランダムな乱数をサンプリングすることで数値計算（例えば、積分計算など）を行う手法の総称→ルコフ決定過程における価値関数の推定問題、方策評価と方策改善問題にも応用出来る。`<br>`

動的計画法と呼ばれる、マルコフ決定過程でモデル化された環境における価値関数のブートストラップ性を利用して、反復的に枝分かを辿りながら価値関数を計算していく手法は、モデルベースの強化学習手法であり、ベルマンの方程式に出てくる状態遷移関数 [![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png)](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が既知でなくてはならないという問題があった。`<br>`

実際の環境が繰り返し生成するエピソードの系列（＝サンプル）を利用して、経験的に価値関数を推定するため、この状態遷移関数が既知でなくてもよいというメリットが存在する。=モデルフリーの強化学習

![1731962720311](image/document/1731962720311.png)

動的計画法の各種手法は、マルコフ決定過程でモデル化した環境が、ベルマン方程式を満たすこと、更に、ベルマン方程式のブーストラップ性により、動的計画法の各種手法のバックアップ線図は、現在の状態行動対 (s,a)∈S×A →次回の状態行動対 (s′,a′)∈S×A で可能な遷移経路全てで分岐していた。

一方、モンテカルロ法では、（モデル化された環境ではなくて）実際の環境上で、１つのエピソードが繰り返し試行され、経験的に１つの経路が繰り返し試行されるだけなので、モンテカルロ法のバックアップ線図は、上図の太線部分で示された１つのエピソードの開始から終端までの１つのサンプリングされた経路となる。

ここで、状態価値関数は、
[![image](https://user-images.githubusercontent.com/25688193/50636865-059c7d00-0f9b-11e9-9c9f-5baa5479c22f.png)](https://user-images.githubusercontent.com/25688193/50636865-059c7d00-0f9b-11e9-9c9f-5baa5479c22f.png)

期待利得に関する期待値で定義されるものであったことを考えると、モンテカルロ法で価値推定を行う場合には、実際の環境が生成するエピソードの系列に従って状態を訪問した後に観測した収益を平均化すれば良い

→より多くの訪問回数を重ねるにつれて、平均値が期待値に収束するようになる。

動的計画はバックアップ木全体を調べる

モンテカルロ法はバックアップ木の特定の経路を調べる

状態推移が不明なため、その時得られた報酬を割引しながら加算していくことで、トータルの報酬を評価することになる。ここが動的計画とは異なる。
